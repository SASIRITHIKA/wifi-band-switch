"""
triband_ann_pso_regularized_v2_wfq.py
Efficient ANN + PSO for Tri-band WiFi Switching (2.4 / 5 / 6 GHz)
Now regularized to avoid unrealistic 100% accuracy.
Added: switching rate (unweighted) and switching rate using a WFQ-style weight column.

WFQ-style switching rate: if the dataset contains a column named one of
['WFQ_Weight','flow_weight','weight','wfq_weight'] that numeric column is used as the per-sample
weight. Otherwise, if 'Channel Utilization (%)' exists it's normalized and used. If neither
exists, equal weights are used (reduces to unweighted).

Weighted switching rate is computed as:
  For each transition i -> i+1, avg_w = (w[i] + w[i+1]) / 2
  weighted_switch_sum = sum( avg_w * 1{pred[i] != pred[i+1]} )
  denom = sum(avg_w) over all transitions
  switching_rate_wfq = weighted_switch_sum / denom

This gives the fraction of (weighted) transitions that were switches.
"""

# --------------------- auto-install required packages if missing ---------------------
import importlib, subprocess, sys, os

REQUIRED = ["numpy", "pandas", "matplotlib", "scikit-learn", "tensorflow", "pyswarms", "joblib"]
for pkg in REQUIRED:
    try:
        importlib.import_module(pkg)
    except Exception:
        print(f"Installing missing package: {pkg} ...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

# --------------------- imports ---------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings, time
warnings.filterwarnings("ignore")

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, precision_score, recall_score,
    roc_curve, auc, precision_recall_curve, average_precision_score
)
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
import pyswarms as ps
import joblib

# --------------------- user settings ---------------------
CSV_PATH = "wifi_dataset.csv"   # dataset path (fallback if not Colab)
TEST_SIZE = 0.25
VAL_SIZE = 0.2
RANDOM_STATE = 42

# Optimized PSO settings
PSO_PARTICLES = 12
PSO_ITERS = 10   # runtime safety

# Mapping dataset columns to normalized metric names
NETWORK_METRIC_MAP = {
    "Throughput (Mbps)": "throughput",
    "Latency (ms)": "latency",
    "Jitter (ms)": "jitter",
    "Packet Loss (%)": "packet_loss",
    "SNR (dB)": "snr",
    "Handoff Delay (ms)": "handoff_delay",
    "Channel Utilization (%)": "channel_utilization",
    "Interference Level (%)": "interference",
    "Switching Rate (unweighted)": "switching_rate_unweighted",
    "Switching Rate (WFQ-weighted)": "switching_rate_wfq"
}

np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

# --------------------- helper functions ---------------------
def build_ann(input_dim, num_classes, hidden1=48, hidden2=24, dropout=0.4, lr=1e-3):
    model = models.Sequential()
    model.add(layers.Input(shape=(input_dim,)))
    model.add(layers.GaussianNoise(0.1))  # stronger noise regularization
    model.add(layers.Dense(int(hidden1), activation='relu',
                           kernel_regularizer=regularizers.l2(1e-3)))
    model.add(layers.Dropout(float(dropout)))
    model.add(layers.Dense(int(hidden2), activation='relu',
                           kernel_regularizer=regularizers.l2(1e-3)))
    model.add(layers.Dropout(float(dropout)))
    model.add(layers.Dense(num_classes, activation='softmax'))
    opt = optimizers.Adam(learning_rate=float(lr))
    # Label smoothing prevents 100% certainty
    model.compile(optimizer=opt,
                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                  metrics=['accuracy'])
    return model

def particle_to_hparams(p):
    h1 = np.clip(p[0], 32, 96)
    h2 = np.clip(p[1], 16, 48)
    dropout = np.clip(p[2], 0.3, 0.6)   # higher dropout range
    lr_exp = np.clip(p[3], -4.5, -3.0)
    lr = 10.0 ** lr_exp
    batch_log = int(np.round(np.clip(p[4], 4, 7)))
    batch = 2 ** batch_log
    return dict(hidden1=int(h1), hidden2=int(h2), dropout=float(dropout), lr=float(lr), batch=int(batch))

def make_fitness(X_train, y_train, X_val, y_val, input_dim, num_classes, max_epochs=5):
    cache = {}
    def fitness_fn(particles):
        losses = np.zeros(particles.shape[0])
        for i, p in enumerate(particles):
            key = tuple(np.round(p, 3))
            if key in cache:
                losses[i] = cache[key]
                continue
            h = particle_to_hparams(p)
            model = build_ann(input_dim, num_classes,
                              hidden1=h['hidden1'], hidden2=h['hidden2'],
                              dropout=h['dropout'], lr=h['lr'])
            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
            model.fit(X_train, y_train, validation_data=(X_val, y_val),
                      epochs=max_epochs, batch_size=h['batch'], verbose=0, callbacks=[es])
            loss, _ = model.evaluate(X_val, y_val, verbose=0)
            losses[i] = loss
            tf.keras.backend.clear_session()
            cache[key] = losses[i]
        return losses
    return fitness_fn

def plot_roc_pr(y_test_onehot, y_score, classes):
    n_classes = y_test_onehot.shape[1]
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_onehot[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{classes[i]} (AUC={roc_auc:.3f})")
    plt.plot([0,1],[0,1],'k--'); plt.title("ROC Curves"); plt.xlabel("FPR"); plt.ylabel("TPR"); plt.legend(fontsize=8)

    plt.subplot(1,2,2)
    for i in range(n_classes):
        prec, rec, _ = precision_recall_curve(y_test_onehot[:, i], y_score[:, i])
        ap = average_precision_score(y_test_onehot[:, i], y_score[:, i])
        plt.plot(rec, prec, label=f"{classes[i]} (AP={ap:.3f})")
    plt.title("Precision-Recall Curves"); plt.xlabel("Recall"); plt.ylabel("Precision"); plt.legend(fontsize=8)
    plt.tight_layout()
    plt.show()

# --------------------- switching rate helper ---------------------
def compute_switching_rates_from_predictions(y_pred, df):
    """Return (unweighted_rate, wfq_weighted_rate).

    Weighted rate uses per-sample weights from the dataset if available; otherwise
    falls back to normalized 'Channel Utilization (%)' if present; otherwise uses equal weights.
    """
    if len(y_pred) <= 1:
        return 0.0, 0.0

    # Unweighted
    transitions = (y_pred[1:] != y_pred[:-1]).astype(int)
    unweighted_rate = transitions.sum() / max(1, len(transitions))

    # Determine weight column
    weight_candidates = ['WFQ_Weight', 'wfq_weight', 'flow_weight', 'weight']
    weight_col = None
    for c in weight_candidates:
        if c in df.columns:
            weight_col = c
            break
    if weight_col is None and 'Channel Utilization (%)' in df.columns:
        # normalize to [0,1]
        arr = pd.to_numeric(df['Channel Utilization (%)'], errors='coerce').fillna(0).values
        if np.nanmax(arr) - np.nanmin(arr) > 0:
            weights = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))
        else:
            weights = np.ones_like(arr)
    elif weight_col is not None:
        weights = pd.to_numeric(df[weight_col], errors='coerce').fillna(0).values
        # if all zeros or negatives, fallback to ones
        if np.allclose(weights, 0) or np.nanmax(weights) <= 0:
            weights = np.ones_like(weights)
    else:
        weights = np.ones(len(df))

    # Ensure length match between predictions and weights
    if len(weights) != len(y_pred):
        # try to align by trimming/padding
        if len(weights) > len(y_pred):
            weights = weights[:len(y_pred)]
        else:
            weights = np.pad(weights, (0, len(y_pred) - len(weights)), 'edge')

    # Compute per-transition average weight
    avg_weights = (weights[1:] + weights[:-1]) / 2.0
    denom = avg_weights.sum() if avg_weights.sum() > 0 else len(avg_weights)
    weighted_switch_sum = (avg_weights * transitions).sum()
    wfq_rate = weighted_switch_sum / denom

    return float(unweighted_rate), float(wfq_rate)

# --------------------- main pipeline ---------------------
def main():
    start_time = time.time()  # <--- start timing

    # --- Colab/Local CSV Handling ---
    csv_path = CSV_PATH
    try:
        import google.colab
        from google.colab import files
        print("Detected Colab â€” please upload CSV (last column must be label):")
        uploaded = files.upload()
        if uploaded:
            csv_path = next(iter(uploaded.keys()))
            print("Uploaded:", csv_path)
    except Exception:
        pass

    if not os.path.exists(csv_path):
        print(f"CSV not found at {csv_path}. Please set CSV_PATH or upload in Colab.")
        return

    # Load dataset
    df = pd.read_csv(csv_path).dropna().reset_index(drop=True)
    print("Loaded dataset:", csv_path, "shape:", df.shape)

    # Features & labels
    X_df = df.iloc[:, :-1].copy()
    y_raw = df.iloc[:, -1].values

    # Encode features
    cat_cols = X_df.select_dtypes(exclude=[np.number]).columns.tolist()
    for col in cat_cols:
        le_feat = LabelEncoder()
        X_df[col] = le_feat.fit_transform(X_df[col].astype(str))
    X = X_df.apply(pd.to_numeric, errors='coerce').fillna(0.0).values

    # Encode labels
    le = LabelEncoder()
    y_enc = le.fit_transform(y_raw)
    classes = le.classes_
    num_classes = len(classes)
    y_onehot = tf.keras.utils.to_categorical(y_enc, num_classes=num_classes)

    # Scale
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Split
    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X_scaled, y_onehot, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_enc, shuffle=True
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_trainval, y_trainval, test_size=VAL_SIZE, random_state=RANDOM_STATE,
        stratify=np.argmax(y_trainval, axis=1), shuffle=True
    )
    input_dim = X.shape[1]

    # PSO optimization
    print("\nStarting PSO optimization...")
    lb = [32, 16, 0.3, -4.5, 4.0]
    ub = [96, 48, 0.6, -3.0, 7.0]
    fitness = make_fitness(X_train, y_train, X_val, y_val, input_dim, num_classes, max_epochs=4)
    optimizer = ps.single.GlobalBestPSO(
        n_particles=PSO_PARTICLES, dimensions=len(lb), options={'c1':0.6,'c2':0.3,'w':0.9},
        bounds=(np.array(lb), np.array(ub))
    )
    cost, pos = optimizer.optimize(fitness, iters=PSO_ITERS, verbose=2)
    best_h = particle_to_hparams(pos)
    print("Best hyperparameters:", best_h)

    # Final model
    final_model = build_ann(input_dim, num_classes,
                            hidden1=best_h['hidden1'], hidden2=best_h['hidden2'],
                            dropout=best_h['dropout'], lr=best_h['lr'])
    cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    history = final_model.fit(
        X_trainval, y_trainval, validation_data=(X_val, y_val),
        epochs=30, batch_size=best_h['batch'], callbacks=[cb], verbose=1
    )

    # Evaluate
    test_loss, test_acc = final_model.evaluate(X_test, y_test, verbose=0)
    y_score = final_model.predict(X_test)
    y_pred = np.argmax(y_score, axis=1)
    y_true = np.argmax(y_test, axis=1)

    print("\n=== MODEL METRICS ===")
    print(f"Test Loss      : {test_loss:.4f}")
    print(f"Test Accuracy  : {test_acc:.4f}")
    print(f"Error Rate     : {1.0-test_acc:.4f}")
    print(f"Macro F1 Score : {f1_score(y_true,y_pred,average='macro'):.4f}")
    print(f"Macro Precision: {precision_score(y_true,y_pred,average='macro'):.4f}")
    print(f"Macro Recall   : {recall_score(y_true,y_pred,average='macro'):.4f}")
    print("Confusion Matrix:\n", confusion_matrix(y_true,y_pred))

    plot_roc_pr(y_test, y_score, classes)

    # Network metrics
    print("\n=== NETWORK METRICS ===")
    # compute switching rates (unweighted and WFQ-weighted)
    unweighted_switch_rate, wfq_switch_rate = compute_switching_rates_from_predictions(y_pred, df)

    for orig_col, norm_key in NETWORK_METRIC_MAP.items():
        if norm_key == "switching_rate_unweighted":
            print(f"{orig_col:35s}: {unweighted_switch_rate:.6f}")
        elif norm_key == "switching_rate_wfq":
            print(f"{orig_col:35s}: {wfq_switch_rate:.6f}")
        elif orig_col in df.columns:
            print(f"{orig_col:35s}: {pd.to_numeric(df[orig_col],errors='coerce').mean():.6f}")
        else:
            print(f"{orig_col:35s}: MISSING")

    # Execution time
    end_time = time.time()
    exec_time = end_time - start_time
    print(f"\n=== EXECUTION TIME ===\nTotal Runtime: {exec_time:.2f} seconds")

if __name__ == "__main__":
    main()
